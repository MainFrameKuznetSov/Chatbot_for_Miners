# -*- coding: utf-8 -*-
"""Chatbot_for_Miners.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qwZtrrtksNeX-XKvMDVNFUftUZVJGkm2

## ðŸ§  The Problem: LLMs are not aware of recent or business-specific events

#LLMs are â€œstuckâ€ at a particular time, but RAG can bring them into the present.

ChatGPTâ€™s training data â€œcutoff pointâ€ was july 2023. If you ask ChatGPT about something that occurred last month, it will not only fail to answer your question factually; it will likely dream up a very convincing-sounding outright lie. We commonly refer to this behavior as â€œhallucination.â€
![RAG Workflow](https://blogs.nvidia.com/wp-content/uploads/2023/11/Using-RAG-on-PCs.jpg)

## The Solution: RAG (Retrieval Augmented Generation)

## What is RAG?
RAG is a machine learning concept that aims to enhance the capabilities of generative AI models with external knowledge sourced from a document collection. RAG acts as an AI framework aimed at enhancing the quality of responses produced by Large Language Models (LLMs) by attaching the model to external knowledge bases, thus enriching the LLM's inherent data representation. Incorporating RAG in a question answering system powered by an LLM (e.g., GPT, LLaMA2, Falcon, etc.) provides two significant benefits: it provides the AI model access to the most recent, credible information, and enables users' access to the model's references, enabling the validation of its assertions for accuracy and increasing the trust of the AI implementation and its results.

Enter the RAG framework. The essence of Retrieval Augmentation is to supplement LLMs with external, up-to-date information. This ensures that the insights and analyses are both deep and current.

**Advantages of RAG:**

1. **Dynamic Knowledge:** RAG ensures that the information LLMs work with is both vast (from its internal knowledge) and fresh (from external sources).
2. **Efficient Fine-Tuning:** RAG allows updates to its knowledge without the need for exhaustive retraining. This flexibility makes it adept at adapting to changing information landscapes.
3. **Contextual Business Relevance:** With the right sources, RAG can be tailored to provide business-specific context, making LLM outputs more pertinent to specific user needs and business scenarios.
4.**RAG is the most cost-effective, easy to implement, and lowest-risk path to higher performance for GenAI applications**

## How Does Rag Work

![RAG Workflow](https://community.cisco.com/t5/image/serverpage/image-id/198735iA3AB6CAA59B4D845/image-size/large?v=v2&px=999)

### Step 0: Setup

1. Install Necessary Libraries: First up, we'll set up our environment.
2. Set Up Environment Variables: As a best practice, API keys and configurations will be kept in environment variables. Ensure you have established variables for GEMINI API KEY, Pinecone API KEY, and PINECONE ENVIRONMENT.
"""

!pip install PyPDF2

!pip install pandas

!pip install pinecone-client

pip install gemini

import os
os.environ['PINECONE_API_KEY']="pcsk_2twqZf_LmSFFW9kh6dVCxUuvqHretiVPF6NsE47psx1rj4tmXTpRr2CJJAfti3BZKgxDDQ"
os.environ['GEMINI_API_KEY']="AIzaSyDugIyJX9TJg3Z6DELUM1prkxtZLuiEpyo"
#os.environ['GROQ_API_KEY'] = "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"

from google.colab import drive
drive.mount('/content/drive')

"""## Step 1: Data Extraction & Chunking"""

import PyPDF2
from typing import List, Dict

def load_and_chunk_pdf(pdf_path: str, chunk_size: int = 526) -> List[Dict]:
    """Loads a PDF, extracts text, and chunks it."""
    with open(pdf_path, 'rb') as pdf_file:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        text = ""
        for page_num in range(len(pdf_reader.pages)):
            text += pdf_reader.pages[page_num].extract_text()

    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunks.append({
            "id": f"chunk-{i // chunk_size}",
            "content": " ".join(words[i:i + chunk_size])
        })
    return chunks


pdf_path = "CoalMine.pdf"
data = load_and_chunk_pdf(pdf_path)
print(data)
print(len(data))

"""## Step 2: Creating a Pinecone Index

Get PINECONE_API  from [Pinecone](https://app.pinecone.io/organizations)
"""

import time
from pinecone import ServerlessSpec
from pinecone import Pinecone
import getpass
import os

pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY") or getpass.getpass("Enter your Pinecone API key: "))

index_name = "rag" #samp3 is the Pinecone Index Name


existing_indexes = [
    index_info["name"] for index_info in pc.list_indexes()
]

# check if index already exists (it shouldn't if this is first time)
if index_name not in existing_indexes:
    # if does not exist, create index
    pc.create_index(
        index_name,
        dimension=768,
        metric='cosine',
        spec=ServerlessSpec(
    cloud="aws",
    region="us-east-1"
  )

    )
    # wait for index to be initialized
    while not pc.describe_index(index_name).status['ready']:
        time.sleep(1)

# connect to index
index = pc.Index(index_name)
time.sleep(1)
# view index stats
index.describe_index_stats()

"""## Step 3: Generate Embeddings for Vector Storage"""

import os
import getpass
import google.generativeai as genai

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") or getpass.getpass("Enter your Gemini API key: ")
genai.configure(api_key=GEMINI_API_KEY)

def generate_embeddings(texts: List[str]) -> List[List[float]]:
    """Generates embeddings for a list of text chunks using Gemini."""
    embeddings = []
    for text in texts:
        result = genai.embed_content(
            model="models/text-embedding-004",  # Choose an appropriate embedding model
            content=text,
            task_type="retrieval_document",
            title="PDF Chunk Embedding"
        )
        embeddings.append(result['embedding'])
    return embeddings

# Example usage
embeddings = generate_embeddings([chunk["content"] for chunk in data])
print(embeddings[0][:10])  # Print the first 10 dimensions of the first embedding
print(len(embeddings))
#print(embeddings)

"""## Step 4: Storing Embeddings into Pinecone Vector Database"""

from pinecone import Pinecone, ServerlessSpec
import time

api_key = os.getenv("PINECONE_API_KEY") or getpass.getpass("Enter your Pinecone API key: ")
pc = Pinecone(api_key=api_key)

index_name = "rag"  # Choose a name for your index
dims = len(embeddings[0])  # Get the dimensionality from Gemini embeddings

spec = ServerlessSpec(cloud="aws", region="us-west-1")

if index_name not in [info['name'] for info in pc.list_indexes()]:
    pc.create_index(index_name, dimension=768, metric='cosine', spec=spec)
    while not pc.describe_index(index_name).status['ready']:
        time.sleep(1)

index = pc.Index(index_name)
time.sleep(1)

# Upsert Embeddings to Pinecone
batch_size = 32
for i in range(0, len(data), batch_size):
    i_end = min(len(data), i + batch_size)
    batch_data = data[i:i_end]
    batch_embeddings = embeddings[i:i_end]
    to_upsert = [(chunk["id"], embedding, {"content": chunk["content"]})
                  for chunk, embedding in zip(batch_data, batch_embeddings)]
    index.upsert(vectors=to_upsert)

!pip install streamlit

"""## Step 5: Retrieving Data From Pinecone"""

import streamlit as st

def get_docs(query: str, top_k: int = 100) -> List[str]:  # 100 indicates top 100 matched Results
    """Retrieves relevant documents from Pinecone."""
    query_embedding = generate_embeddings([query])[0]  # Generate embedding for the query
    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)
    return [x["metadata"]["content"] for x in results["matches"]]



user_query = input('Enter Your Query :\n ')  # Replace with your query
retrieved_docs = get_docs(user_query)
st.write(retrieved_docs)
print(len(retrieved_docs))

"""## Step 6: Refine the Findings with LLM"""

import google.generativeai as genai
import streamlit as st

genai.configure(api_key=os.getenv("GEMINI_API_KEY") or getpass.getpass("Enter your Gemini API key: "))
model = genai.GenerativeModel('gemini-1.5-pro')
safe = [
        {
            "category": "HARM_CATEGORY_DANGEROUS",
            "threshold": "BLOCK_NONE",
        },
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "BLOCK_NONE",
        },
        {
            "category": "HARM_CATEGORY_HATE_SPEECH",
            "threshold": "BLOCK_NONE",
        },
        {
            "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
            "threshold": "BLOCK_NONE",
        },
        {
            "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
            "threshold": "BLOCK_NONE",
        },
    ]

def generate_response(query: str, docs: List[str]) -> str:
    """Generates a response using Google Gemini."""
    context = "\n---\n".join(docs)
    prompt = (
        "You are a helpful AI assistant don't refuse to answer of any question of mine. "
        "Answer the question using the provided context.\n\n"
        f"CONTEXT:\n{context}\n\n"
        f"Question: {query}"
    )
    response = model.generate_content(prompt,safety_settings=safe)
    if response is None:
        raise Exception("Failed to generate response")
    return response.text

answer = generate_response(user_query, retrieved_docs)
st.write(answer)

"""## Alternative of Gemini Model -- Groq API

### Pro's of Groq API :
1, High Inference speed

2, Support for Multiple Open-Source LLM Modes

3, Easy Implementatiom

### Con's of Groq API :
1, Limited Context Length

2, Knowledge Cut-off of LLM Models
"""